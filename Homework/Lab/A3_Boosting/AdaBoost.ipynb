{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quick introduction to jupyter notebooks**\n",
    "* Each cell in this notebook contains either code or text.\n",
    "* You can run a cell by pressing Ctrl-Enter, or run and advance to the next cell with Shift-Enter.\n",
    "* Code cells will print their output, including images, below the cell. Running it again deletes the previous output, so be careful if you want to save some results.\n",
    "* You don't have to rerun all cells to test changes, just rerun the cell you have made changes to. Some exceptions might apply, for example if you overwrite variables from previous cells, but in general this will work.\n",
    "* If all else fails, use the \"Kernel\" menu and select \"Restart Kernel and Clear All Output\". You can also use this menu to run all cells.\n",
    "* A useful debug tool is the console. You can right-click anywhere in the notebook and select \"New console for notebook\". This opens a python console which shares the environment with the notebook, which let's you easily print variables or test commands.\n",
    "\n",
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io as sio\n",
    "\n",
    "from utils import GenerateHaarFeatureMasks, GenerateTrainTestData, ExtractHaarFeatures\n",
    "from utils import PlotErrorGraphs, PlotClassifications, PlotSelectedHaarFeatures, PlotHaarFeatureDemonstration, PlotSolvayHeatmap\n",
    "plt.rcParams['figure.facecolor']='white'\n",
    "\n",
    "# Load data\n",
    "faces = sio.loadmat('Data/faces.mat')['faces'].astype(\"float\")\n",
    "nonfaces = sio.loadmat('Data/nonfaces.mat')['nonfaces'].astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***! IMPORTANT NOTE !***\n",
    "\n",
    "Your implementation should only use the numpy module (already imported as np). The numpy module provides all the functionality you need for this assignment and makes easier debuging your code. No other modules, e.g. scikit-learn or scipy among others, are allowed and solutions using modules other than the numpy module will be sent for re-submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **1 Introduction**\n",
    "In this assignment you will explore a branch of supervised learning called *Ensemble lerning*, specifically using the algorithm known as *AdaBoost*. This algorithm trains multiple simple *weak classifiers* that are later combined into a *strong classifier* which is significantly more powerful than the individual *weak classifiers*. The *weak classifier* you will work with is a simple type of binary classifier called decision stump. These classify scalar inputs based on a single comparisson with a threshold.\n",
    "\n",
    "![](NotebookMaterials/DecisionStump.png \"Decision stump\")\n",
    "\n",
    "In order to apply this classifier to images, we require a way to describe the features of the images using a single number that the decision stump can compare to the threshold. In a seminal article by Viola and Jones, called [Robust real-time object detection](https://www.researchgate.net/publication/215721846_Robust_Real-Time_Object_Detection), this was demonstrated using so called Haar-features, which is what we will use in this assignment.\n",
    "\n",
    "A Haar-feature is a simple yet surprisingly effective way to quantify edges and gradients in images, by computing the weighted sum of different areas of the image. We will begin by exploring how these Haar-features work, in order to understand the fundamental building blocks of the algorithm you will implement later.\n",
    "\n",
    "Run the following code cell to generate a random Haar-feature, select a random face and nonface image, and compute the feature values by applying the Haar-feature to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random Haar-feature and select a random face and a random non-face\n",
    "randomHaarFeatureMask = GenerateHaarFeatureMasks(1)[:,:,0]\n",
    "randomFace = faces[:,:,np.random.randint(faces.shape[2])]\n",
    "randomNonFace = nonfaces[:,:,np.random.randint(nonfaces.shape[2])]\n",
    "\n",
    "PlotHaarFeatureDemonstration(haar=randomHaarFeatureMask, face=randomFace, nonface=randomNonFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Haar-features we generate in this assignment consist of either two or three areas, each with a scaling factor of -1, 1, or 2. The pixels outside the Haar-feature has a scaling of 0. The images are pixel-wise multiplied with the Haar-feature, after which the entire image is summed to a single scalar value. This weighted sum of pixels is the feature value of the image, which can be used in the weak classifier.\n",
    "\n",
    "Of course, since the Haar-feature in this picture is chosen entierly at random, we should not expect that it makes sense as a good feature extractor. Finding a good set of Haar-features is your task, but we will get back to that soon. Before that, take a look at the decision stump classifier again. Now that we know how to extract a single feature value from an image, we can see how a desision stump can be used to classify the images as faces or nonfaces. We can write the decision stump as a function:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "f(x \\: | \\: \\tau) =\n",
    "\\begin{cases}\n",
    "    +1 & \\text{if } x \\geq \\tau \\\\\n",
    "    -1 & \\text{if } x < \\tau\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "As you have already seen in the lecture on boosting and ensemble learning, this can also be extended to a more general definition by including a polarity parameter:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "f(x \\: | \\: \\tau, p) =\n",
    "\\begin{cases}\n",
    "    +1 & \\text{if } px \\geq p\\tau \\\\\n",
    "    -1 & \\text{if } px < p\\tau\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "By selecting $p$ equal to +1 or -1, we can flip the sign of the predicted output. Since this is a binary classification, this also changes the error of the classifier from $e$ to $1-e$. This is necessary if we want to guarantee that we can find the optimal classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1 Generating the training data**\n",
    "Since a single random Haar-feature is unlikely to be a good feature extractor, we need to generate a large number of them and find the ones that are useful. But extracting the feature values from the images during training is very inefficient, so instead we precompute the feature value for each Haar-feature applied on each image. This will therefore result in a feature value matrix $X^{[R,C]}$ (i.e. $R$ rows and $C$ columns), where $R$ is the number of Haar-feature and $C$ is the number of images in the dataset, as illustrated in this picture:\n",
    "\n",
    "![](NotebookMaterials/HaarExtractionDemo.png \"Precomputing feature values\")\n",
    "\n",
    "Note that the training loop will only work with this matrix, never with the original images or Haar-features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2 The AdaBoost algorithm**\n",
    "AdaBoost is an ensemble learning algorithm that successively trains many weak classifiers that are informed about the weaknesses of the previous classifiers. This is done by keeping track of a weight value $d$ for each training image, representing how \"difficult\" that image has been to classify during training so far. After a weak classifier has been trained, the predictions of that classifier is used to update the weights of all images. If an image was classified correctly the weight is exponentially decreased, and the next weak classifier will therefore put less emphasis on that sample since it is already taken care of by the previous weak classifiers. The weights of misclassifed images are instead exponentially increased to make them more important for the next weak classifier.\n",
    "\n",
    "In more detail, if a weak classifier results in an error $\\varepsilon$, we compute the update factor\n",
    "\n",
    "$$\\large \\alpha = \\frac{1}{2} \\mathrm{ln}\\left( \\frac{1-\\varepsilon}{\\varepsilon} \\right)$$\n",
    "\n",
    "and then update the weights according to\n",
    "\n",
    "$$\\large d_{t+1} = \\begin{cases} d_t e^{-\\alpha} & \\text{if correctly classified} \\\\ d_t e^{\\alpha} & \\text{if misclassified} \\end{cases}$$\n",
    "\n",
    "We also normalize $d_{t+1}$ such that the total weight of all samples is 1. The next weak classifier is then trained using the new weights $d_{t+1}$, which will put the focus on missclassified images and therefore result in a new combination of optimal parameters. This is how AdaBoost gradually covers more and more of the difficult cases in the data, even thought the same training images and Haar-feature are used for each weak classifier.\n",
    "\n",
    "When the training of all weak classifiers is finished we use them in a weighted voting scheme to build the *strong classifier*.\n",
    "\n",
    "$$\\large H(x) = \\mathrm{sign} \\left( \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right)$$\n",
    "\n",
    "Note that we use the update factor $\\alpha$ for each weak classifier as a weight in the voting. This is because weak classifiers with lower total error should have more voting power to increase the overall performance of the strong classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 1:</span>**\n",
    "The error of a single weak classifier is always in the range 0 to 0.5 (since we otherwise flip the polarity). Consider the two edge cases:\n",
    "1. If you get an optimal $\\varepsilon = 0$ this single weak classifier correctly predicted the entire training set. What does this indicate about your hyperparameters (number of Haar-features, number of training images, number of weak classifiers)?\n",
    "2. If you get an optimal $\\varepsilon = 0.5$ the training will get stuck. Explain why this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3 Pseudo-code**\n",
    "With all this in mind, we can summarize the entire algorithm in the following pseudo-code:\n",
    "\n",
    "`TrainWeakClassifier:`<br>\n",
    ">`for every combination of (feature, threshold, polarity):`<br>\n",
    ">> `Run WeakClassifier`<br>\n",
    ">> `Measure error`<br>\n",
    ">> `if new smallest error:`<br>\n",
    ">>> `Save parameters`<br>\n",
    ">\n",
    "> `return optimal parameters`\n",
    "\n",
    "<br>\n",
    "\n",
    "`TrainStrongClassifer:`<br>\n",
    "> `Initialize weights for each training sample`<br>\n",
    "> `for number of weak classifiers:`<br>\n",
    ">> `TrainWeakClassifier`<br>\n",
    ">> `Measure error and get predictions`<br>\n",
    ">> `Compute update parameter alpha`<br>\n",
    ">> `Update training sample weights`<br>\n",
    ">\n",
    ">`return optimal parameters`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2. Implementing the weak classifier**\n",
    "\n",
    "You will now start by implementing the decision stump weak classifier, the weighted error function, and the training loop for a single weak classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 Weak classifier forward function**\n",
    "\n",
    "The forward function classifies the data according to the threshold and polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement decision stump classifier\n",
    "# X - Feature values (vector)\n",
    "# t - Threshold (scalar)\n",
    "# p - Polarity (1 or -1)\n",
    "# Return a vector of predicted classes (1 or -1), same shape as X.\n",
    "\n",
    "def WeakClassifier(X,t,p):\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 Weighted classification error**\n",
    "\n",
    "This function computes the weighted classification error based on predicted and true classes, as well as the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement error function\n",
    "# Y  - Target classes (vector)\n",
    "# Yp - Predicted classes (vector)\n",
    "# D  - Weights (vector)\n",
    "# Return a scalar for the total weighted error of all predictions.\n",
    "\n",
    "def WeakClassifierError(Y,Yp,D):\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3 Training function for a single weak classifier**\n",
    "\n",
    "Here you should implement the training of a single weak classifier, using the two functions you just implemented. Based on the training data, sample weights, and target labels, find the optimal way to separate the two classes. You should do this using brute force search, in other words, try every (reasonable) combination of parameters, measure the errors, and pick the best. Return all parameters that define the optimal weak classifier, as well as the error of said weak classifier.\n",
    "\n",
    "*Tip: Brute force search is not an efficient algorithm, so any optimization you can do is welcome. When you have a working version of this function, consider if there is any way you can decrease the amount of calls to WeakClassifier, especially by reducing the number of thresholds. Of course, you must still guarantee that you can find the optimal parameters, so for example naively discarding every other threshold will not work.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement function for training one decision stump\n",
    "# X - Training data (matrix)\n",
    "# Y - Target classes (vector)\n",
    "# D - Error weights (vector)\n",
    "# Return optimal feature, threshold, polarity, and the corresponding error\n",
    "\n",
    "def TrainWeakClassifier(X,Y,D):\n",
    "    \n",
    "    fOpt = None\n",
    "    tOpt = None\n",
    "    pOpt = None\n",
    "    eMin = np.inf\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ============================================\n",
    "            \n",
    "    return fOpt, tOpt, pOpt, eMin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running the following minimal test case. You should find that the optimal feature is 1 (remember that python is 0-indexed!), the optimal threshold is between 1 and 3 (depending on your implementation), and the optimal polarity is -1. The minimum error should be 0.222 repeating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2, 2, 2, 2, 2, 3], [3, 3, 3, 1, 1, 4], [4, 2, 4, 2, 4, 2]])\n",
    "Y = np.array([-1,-1, -1, 1, 1, 1])\n",
    "D = np.array([1,1,1,2,2,2]) / 9\n",
    "\n",
    "fOpt, tOpt, pOpt, eMin = TrainWeakClassifier(X,Y,D)\n",
    "print(f\"Optimal feature   : {fOpt}\")\n",
    "print(f\"Optimal threshold : {tOpt}\")\n",
    "print(f\"Optimal polarity  : {pOpt}\")\n",
    "print(f\"Minumum error     : {eMin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3. Implementing the strong classifier**\n",
    "You should now implement the main AdaBoost algorithm, using the training function for weak classifiers you just implemented. Return the optimal parameters for all weak classifiers. We have given you some boilerplate code here to help you get started, and to provide some useful outputs during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter as tic\n",
    "from datetime import timedelta\n",
    "\n",
    "# Implement AdaBoost training\n",
    "# X - Training data (matrix)\n",
    "# Y - Target classes (vector)\n",
    "# N - Number of weak classifiers to train (scalar)\n",
    "# Return the optimal features, thresholds, polarities, and alphas describing the N weak classifiers\n",
    "\n",
    "def TrainStrongClassifier(X,Y,N=10):\n",
    "    \n",
    "    fOpt = np.zeros(N,\"int\")\n",
    "    tOpt = np.zeros(N)\n",
    "    pOpt = np.zeros(N)\n",
    "    aOpt = np.zeros(N)\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # Initialize sample weights here\n",
    "    D = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    timeStart = tic()\n",
    "    \n",
    "    for n in range(N):\n",
    "        \n",
    "        # --------------------------------------------\n",
    "        # === Your code here =========================\n",
    "        # --------------------------------------------\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ============================================\n",
    "        \n",
    "        # This prints the training progress in a nice format\n",
    "        timeLeft = round((tic()-timeStart)*(N/(n+1) - 1))\n",
    "        etaStr = str(timedelta(seconds=timeLeft))\n",
    "        print(f\"{n+1} of {N}: ETA {etaStr}    \", end=\"\\r\")\n",
    "    \n",
    "    return fOpt, tOpt, pOpt, aOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you should implement the StrongClassifier function. This function takes in the parameters returned from the training function and classifies the input $X$. If input $Y$ is also provided, the error of the strong classifier should also be computed and returned.\n",
    "\n",
    "Additionally, it is interesting to evaluate the strong classifier not only on the final ensemble model, but how the performance improved during the training, as more and more weak classifiers were added. As such, the error returned from this function should be a vector with the same lenght as the number of weak classifiers. The first value in the vector should only use the first weak classifier. The second value should use the first two classifiers in the ensemble, and so on. In general, the n:th entry is the error when using the first n weak classifiers as a strong classifier. This way we can plot the performance as a function of number of weak classifiers, which is important to investigate, for example, overfitting.\n",
    "\n",
    "*Tip: There is a function in numpy called* `cumsum`*, which stands for cumulative sum. It is not mandatory to use this, but the code for computing the error vector can be very short if you do.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement strong classifier\n",
    "# Params - Parameters of the weak classifiers (output from AdaBoost training function)\n",
    "# X - Data to classify (matrix)\n",
    "# Y - Target labels (vector). Optional input, see text description above.\n",
    "# Return predicted classes (vector) and optionally the error (vector).\n",
    "\n",
    "def StrongClassifier(Params, X, Y=None):\n",
    "    \n",
    "    H = None\n",
    "    Err = None\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return H, Err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **4. Training and evaluating the model**\n",
    "Now you will run the AdaBoost training and evaluate the results. We have provided all required code for plotting the results, but if there are additional results you wish to show you are welcome to make your own plots as well.\n",
    "\n",
    "The following cell generates a set of random Haar-features, then precomputes the feature values and splits the data into a training set and a test set. Change the three hyperparamters; number of Haar-features, number of training samples, and number of weak classifiers to obtain the target performance 7% error or lower. Of course the error should be stable below the target as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Haar feature masks\n",
    "haarFeatureMasks = GenerateHaarFeatureMasks(nbrHaarFeatures = ???)\n",
    "\n",
    "# Extract feature values and split into training and test sets\n",
    "trainImages, testImages, XTrain, YTrain, XTest, YTest = GenerateTrainTestData(faces, nonfaces, haarFeatureMasks, nbrTrainImages = ???)\n",
    "\n",
    "# Train the classifier\n",
    "optParams = TrainStrongClassifier(XTrain, YTrain, N = ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate your classifier on the training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTrain, ErrTrain = StrongClassifier(optParams, XTrain, Y = YTrain)\n",
    "HTest , ErrTest  = StrongClassifier(optParams, XTest , Y = YTest)\n",
    "\n",
    "ErrTrain *= 100\n",
    "ErrTest  *= 100\n",
    "\n",
    "print(f\"Training error: {ErrTrain[-1]:.1f}%\")\n",
    "print(f\"Test error: {ErrTest[-1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the error of the strong classifier as a function of the number of weak classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotErrorGraphs(ErrTrain, ErrTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 2:</span>**\n",
    "Suppose you want to deploy your model in a real-world application, for example in a smartphone camera app that can highlight faces. Based on the above result, how many weak classifiers would you choose to include in the model? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 3:</span>**\n",
    "Discuss your choise of hyperparameters\n",
    "* number of training images\n",
    "* number of Haar-features\n",
    "* number of weak classifiers\n",
    "\n",
    "and how each affect the model performance. What are the advantages and disadvantages to low and high values for each parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1 Investigate the trained model**\n",
    "Plot some examples of faces and nonfaces that were classified correctly and that were misclassified. To do this, we requre you to find the index in the test data of these four cases:\n",
    "* Faces classified as faces (F_F)\n",
    "* Faces classified as nonfaces (F_NF)\n",
    "* Nonfaces classified as faces (NF_F)\n",
    "* Nonfaces classified as nonfaces (NF_NF)\n",
    "\n",
    "There are many ways to do this. It is possible to write each in a single line using numpy functions, although this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_F   = ???\n",
    "F_NF  = ???\n",
    "NF_F  = ???\n",
    "NF_NF = ???\n",
    "\n",
    "PlotClassifications(testImages, F_F, F_NF, NF_NF, NF_F, N=3, selectRandom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 4:</span>**\n",
    "Run the above cell a few times and discuss some potential issues in the data that might explain these misclassified images. Can you think of any pre-processing that might improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to plot (some) of your selected Haar-features. Feel free to change the paramters `shuffle` and `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs:\n",
    "# - haarFeatureMasks: The list of all generated Haar features\n",
    "# - selectedIdx: The feature numbers of Haar features selected in the training\n",
    "# - shuffle (False/True): Select in random order or as they were selected in training\n",
    "# - N: The number of Haar features to plot\n",
    "\n",
    "PlotSelectedHaarFeatures(haarFeatureMasks, selectedIdx = optParams[0], shuffle = False, N = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 5:</span>**\n",
    "Choose one or two of these Haar-features and speculate what feature of a face this might detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **5. Applying the model on real data**\n",
    "Evaluating on the test data is, of course, a valid method to measure the objective performance of the model. However, if doesn't really give a feel for how useful the model would actually be in a real world application. This final test applies the model to every 24x24 pixel subimage in this famous photograph of the 1927 Solvay Conference on Physics, and highlights the most likely parts of the image to contain faces.\n",
    "\n",
    "![](NotebookMaterials/Solvay.png \"Famous photo from the 1927 Solvay Conference on Physics.\")\n",
    "\n",
    "Let's see how many of these famous faces your model can find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvay = sio.loadmat('Data/solvay.mat')['X'].astype(\"float\")\n",
    "xSolvay = ExtractHaarFeatures(solvay, haarFeatureMasks)\n",
    "\n",
    "cSolvay = StrongClassifier(optParams, xSolvay)\n",
    "\n",
    "PlotSolvayHeatmap(cSolvay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 6:</span>**\n",
    "Give a final discussion based on all the results. Do you think the model works well? Can you think of any way to improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd50aace418a96e8a4fe691a4d2292bd7058ca4eeebcf0b6e2084f539c4e7b28"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
