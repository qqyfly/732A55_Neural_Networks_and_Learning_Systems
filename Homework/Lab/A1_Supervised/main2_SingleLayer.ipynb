{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quick introduction to jupyter notebooks**\n",
    "* Each cell in this notebook contains either code or text.\n",
    "* You can run a cell by pressing Ctrl-Enter, or run and advance to the next cell with Shift-Enter.\n",
    "* Code cells will print their output, including images, below the cell. Running it again deletes the previous output, so be careful if you want to save some results.\n",
    "* You don't have to rerun all cells to test changes, just rerun the cell you have made changes to. Some exceptions might apply, for example if you overwrite variables from previous cells, but in general this will work.\n",
    "* If all else fails, use the \"Kernel\" menu and select \"Restart Kernel and Clear All Output\". You can also use this menu to run all cells.\n",
    "* A useful debug tool is the console. You can right-click anywhere in the notebook and select \"New console for notebook\". This opens a python console which shares the environment with the notebook, which let's you easily print variables or test commands.\n",
    "\n",
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when changed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Plot figures \"inline\" with other output\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules, classes, functions\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import loadDataset, splitData, plotProgressNetwork, plotProgressOCR, \\\n",
    "    plotResultsDots, plotIsolines, plotConfusionMatrixOCR, plotResultsDotsGradient\n",
    "from evalFunctions import calcAccuracy, calcConfusionMatrix\n",
    "\n",
    "# Configure nice figures\n",
    "plt.rcParams['figure.facecolor']='white'\n",
    "plt.rcParams['figure.figsize']=(8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***! IMPORTANT NOTE !***\n",
    "\n",
    "Your implementation should only use the `numpy` (`np`) module. The `numpy` module provides all the functionality you need for this assignment and makes it easier debuging your code. No other modules, e.g. `scikit-learn` or `scipy` among others, are allowed and solutions using modules other than `numpy` will be sent for re-submission. You can find everything you need about `numpy` in the official [documentation](https://numpy.org/doc/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **1. Single layer neural network**\n",
    "\n",
    "In this and the next notebook, you will implement and train two types of neural networks. We begin with a single layer network in this notebook, where the predicted classes are based on linear combinations of the input features. The single layer network can only learn decision boundaries. The next notebook introduces the multi-layer network, which is a general function approximator and therefore can learn any function given enough resourses.\n",
    "\n",
    "Training a network consist of three main steps:\n",
    "1. Forward pass - Comupte predicted outputs based on inputs features.\n",
    "2. Backward pass - Compute weight gradients.\n",
    "3. Update - Use gradients and hyperparameters to update weights.\n",
    "\n",
    "Begin by implementing these three functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1 Implementing the forward pass**\n",
    "\n",
    "In the following function you will implement the forward pass, i.e. take the input features `X`, weights `W`, and biases `B`, and compute the predicted outputs of the network. Optionally, you can choose to also implement the forward pass when using *tanh* activation function in the output layer. This does not make the classifier non-linear, but it can help speed up the training.\n",
    "\n",
    "Note: In `numpy`, the multiplication symbol `*` means element-wise multiplication. Matrix multiplication is done using the `@` symbol, for example `A @ B` where `A` and `B` are compatible matrices. Transposing a `numpy` matrix is done with `A.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, B, useTanhOutput=False):\n",
    "    \"\"\"Forward pass of single layer network.\n",
    "\n",
    "    Performs one forward pass of the single layer network, i.e\n",
    "    it takes the input data and calculates the output for each sample.\n",
    "\n",
    "    Args:\n",
    "        X (array): Input samples.\n",
    "        W (array): Neural network weights.\n",
    "        B (array): Neural network biases.\n",
    "\n",
    "    Returns:\n",
    "        Y (array): Output for each sample and class.\n",
    "        L (array): Resulting label of each sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    Y = ???\n",
    "        \n",
    "    # ============================================\n",
    "        \n",
    "    # Calculate labels\n",
    "    L = np.argmax(Y, axis=1)\n",
    "    \n",
    "    return Y, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2 Implementing the backward pass**\n",
    "\n",
    "Now, using the predicted outputs `Y` from the forward pass, compute the gradients of the weights and biases. To do this you must use the target outputs `D`. If you print `D` you will see that it contains values that are ±0.99. This might seem strange at first, where ±1 would be a more intuitive choice. However, this is a trick to prevent the weights of the network from becomming too large when using the optional *tanh* activation in the output layer. Since *tanh* never reach ±1 (only in the limit at ±Inf) it would push the weight to very large values in order to reach closer and closer to those limits. This is undesirable, so instead we choose the targets ±0.99, which only require the inputs to *tanh* to be approximately ±2.6, allowing much smaller weights and therefore a more stable classifier. None of this should matter for your implementation of the backward pass, but it is nonetheless good to understand the data you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(W, B, X, Y, D, useTanhOutput=False):\n",
    "    \"\"\"Compute the gradients for network weights and biases\n",
    "\n",
    "    Args:\n",
    "        W (array): Current values of the network weights.\n",
    "        B (array): Current values of the network biases.\n",
    "        X (array): Training samples.\n",
    "        Y (array): Predicted outputs.\n",
    "        D (array): Target outputs.\n",
    "        \n",
    "        useTanhOutput (bool) (optional):\n",
    "            True  - Network uses tanh activation on output layer\n",
    "            False - Network uses linear (no) activation on output layer\n",
    "        \n",
    "    Returns:\n",
    "        GradW (array): Gradients with respect to W\n",
    "        GradB (array): Gradients with respect to B\n",
    "    \"\"\"\n",
    "    \n",
    "    N = Y.shape[0]\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "        \n",
    "    # Calculate gradients\n",
    "    GradW = ???\n",
    "    GradB = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return GradW, GradB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3 Implementing the weight update**\n",
    "\n",
    "Finally, after computing the gradients using the `backward` function, update and return the new weights and biases. While there are many advanced updated methods, we will only look at the most basic in this assignment,  unmodified gradient descent. Implement the following function, which should be very short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, B, GradW, GradB, params):\n",
    "    \"\"\"Update weights and biases using computed gradients.\n",
    "\n",
    "    Args:\n",
    "        W (array): Current values of the network weights.\n",
    "        B (array): Current values of the network biases.\n",
    "        GradW (array): Gradients with respect to W.\n",
    "        GradB (array): Gradients with respect to B.\n",
    "        \n",
    "        params (dict):\n",
    "            - learningRate: Scale factor for update step.\n",
    "        \n",
    "    Returns:\n",
    "        W (array): Updated weights.\n",
    "        B (array): Updated biases.\n",
    "    \"\"\"\n",
    "    \n",
    "    LR = params[\"learningRate\"]\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    W = ???\n",
    "    B = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return W, B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4 The training function**\n",
    "\n",
    "In order to train the network using your implementation of `forward`, `backward`, and `update`, we have prepared the following function for you. This takes your training data and initial weights, sets up all required variables, then trains the network for the specified number of epochs while tracking metrics and plotting the training progress. Read the function and make sure you understand how your own implementations are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSingleLayer(XTrain, DTrain, XTest, DTest, W0, B0, params, plotProgress=True):\n",
    "    \"\"\"Trains a single-layer network.\n",
    "\n",
    "    Args:\n",
    "        XTrain (array): Training samples.\n",
    "        DTrain (array): Training network target values.\n",
    "        XTest (array): Test samples.\n",
    "        DTest (array): Test network target values.\n",
    "        W0 (array): Initial values of the network weights.\n",
    "        B0 (array): Initial values of the network biases.\n",
    "        \n",
    "        params (dict): Dictionary containing:\n",
    "            epochs (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "            useTanhOutput (bool): Determines if output layer should use tanh activation.\n",
    "\n",
    "    Returns:\n",
    "        W (array): Weights after training.\n",
    "        B (array): Biases after training.\n",
    "        metrics (dict): Losses and accuracies for training and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    metrics = {keys:np.zeros(params[\"epochs\"]+1) for keys in [\"lossTrain\", \"lossTest\", \"accTrain\", \"accTest\"]}\n",
    "    \n",
    "    if \"useTanhOutput\" not in params:\n",
    "        params[\"useTanhOutput\"] = False\n",
    "\n",
    "    nTrain = XTrain.shape[0]\n",
    "    nTest  = XTest.shape[0]\n",
    "\n",
    "    # Set initial weights\n",
    "    W = W0\n",
    "    B = B0\n",
    "\n",
    "    # Get class labels\n",
    "    LTrain = np.argmax(DTrain, axis=1)\n",
    "    LTest  = np.argmax(DTest , axis=1)\n",
    "\n",
    "    # Calculate initial metrics\n",
    "    YTrain, LTrainPred = forward(XTrain, W, B, params[\"useTanhOutput\"])\n",
    "    YTest , LTestPred  = forward(XTest , W, B, params[\"useTanhOutput\"])\n",
    "    \n",
    "    # Including the initial metrics makes the progress plots worse, set nan to exclude\n",
    "    metrics[\"lossTrain\"][0] = np.nan #((YTrain - DTrain)**2).mean()\n",
    "    metrics[\"lossTest\"][0]  = np.nan #((YTest  - DTest )**2).mean()\n",
    "    metrics[\"accTrain\"][0]  = np.nan #(LTrainPred == LTrain).mean()\n",
    "    metrics[\"accTest\"][0]   = np.nan #(LTestPred  == LTest ).mean()\n",
    "\n",
    "    # Create figure for plotting progress\n",
    "    if W0.shape[0] < 64:\n",
    "        fig = plt.figure(figsize=(20,8), tight_layout=True)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20,8), constrained_layout=True)\n",
    "\n",
    "    # Training loop\n",
    "    for n in range(1, params[\"epochs\"]+1):\n",
    "        \n",
    "        # --------------------------------------------\n",
    "        # === This is the important part =============\n",
    "        # === where your code is applied =============\n",
    "        # --------------------------------------------\n",
    "        \n",
    "        # Compute gradients...\n",
    "        GradW, GradB = backward(W, B, XTrain, YTrain, DTrain, params[\"useTanhOutput\"])\n",
    "        # ... and update weights\n",
    "        W, B = update(W, B, GradW, GradB, params)\n",
    "        \n",
    "        # ============================================\n",
    "        \n",
    "        # Evaluate errors\n",
    "        YTrain, LTrainPred = forward(XTrain, W, B, params[\"useTanhOutput\"])\n",
    "        YTest , LTestPred  = forward(XTest , W, B, params[\"useTanhOutput\"])\n",
    "        metrics[\"lossTrain\"][n] = ((YTrain - DTrain)**2).mean()\n",
    "        metrics[\"lossTest\"][n]  = ((YTest  - DTest )**2).mean()\n",
    "        metrics[\"accTrain\"][n]  = (LTrainPred == LTrain).mean()\n",
    "        metrics[\"accTest\"][n]   = (LTestPred  == LTest ).mean()\n",
    "\n",
    "        # Plot progress\n",
    "        if (plotProgress and not n % (params[\"epochs\"] // 25)) or n == params[\"epochs\"]:\n",
    "            if W0.shape[0] < 64:\n",
    "                plotProgressNetwork(fig, W, B, metrics, n=n, cmap='coolwarm')\n",
    "            else:\n",
    "                plotProgressOCR(fig, W, metrics, n=n, cmap='coolwarm_r')\n",
    "\n",
    "    return W, B, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.5. Test your implementation**\n",
    "\n",
    "It is time to test your implementation by training the a network on the first dataset. You will begin by running all the required code manually, so you will see and understand each part of the process. Later, we will define a function that does all these steps for you. But for now, run the following cell to load the data and split it into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and load dataset\n",
    "datasetNr = 1\n",
    "X, D, L = loadDataset(datasetNr)\n",
    "\n",
    "# Split data into training and test sets\n",
    "XTrain, DTrain, LTrain, XTest, DTest, LTest = splitData(X, D, L, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important aspect of data preprocessing is data normalization. Often, normalizing the data can turn a very difficult dataset into a simple one, simply by ensuring that the data is similarly distributed in each feature dimension. This becomes much more important as we increase the complexity of the data, so you might not see much difference in the first three datasets. However, in the OCR data this will be much more important, and even more so in the next assignement on deep learning.\n",
    "\n",
    "For now, we define a function that normalizes each feature to have zero mean and unit standard deviation\n",
    "\n",
    "$$ \\large m = \\frac{1}{N} \\sum_i^N X_i \\quad \\quad \\quad \\large s = \\sqrt{ \\frac{1}{N} \\sum_i^N \\left( X_i - m \\right)^2 }$$\n",
    "\n",
    "$$ \\large X_\\mathrm{norm} = \\frac{X - m}{s}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    # Compute mean and std\n",
    "    m = X.mean(axis=0)\n",
    "    s = X.std(axis=0)\n",
    "    # Prevent division by 0 is feature has no variance\n",
    "    s[s == 0] = 1\n",
    "    # Return normalized data\n",
    "    return (X - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code to normalize the training and test data. These are the datasets you will input to the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainNorm = normalize(XTrain)\n",
    "XTestNorm  = normalize(XTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should initialize your weights and biases. It is up to you to figure out the correct shapes of `W0` and `B0` based on the number of inputs and number of output classes. You also need to experiment with different initialization strategies. For example, should the weights be all zeros, all ones, random, and in that case what distribution and magnitude? The only requirement we give you is that `W0` and `B0` are numpy arrays, to make the rest of the code work as expected.\n",
    "\n",
    "*Hint: The question how to initialize the network parameters is not new, and many clever researchers have tried to come up with automatic solutions. If you are interested you can search for \"Xavier initialization\" and try to implement it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the training parameters in the `params` dictionary. Later, you will add some more parameters, but for this first test the number of `epochs` and the `learningRate` is sufficient. Then run the training and observe how the losses, accuracies, and network weights change as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"epochs\": 1000, \"learningRate\": 0.05}\n",
    "W, B, metrics = trainSingleLayer(XTrainNorm, DTrain, XTestNorm, DTest, W0, B0, params, plotProgress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the trained weights to predict the output labels. To investigate overfitting, we predict both the training and test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LPredTrain = forward(XTrainNorm, W, B)[1]\n",
    "LPredTest  = forward(XTestNorm , W, B)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the labels to get accuracies and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training and test accuracy\n",
    "accTrain = calcAccuracy(LPredTrain, LTrain)\n",
    "accTest = calcAccuracy(LPredTest, LTest)\n",
    "print(f'Train accuracy: {accTrain:.4f}')\n",
    "print(f'Test accuracy: {accTest:.4f}')\n",
    "\n",
    "# Calculate confunsion matrix of test data\n",
    "confMatrix = calcConfusionMatrix(LPredTest, LTest)\n",
    "print(\"Test data confusion matrix:\")\n",
    "print(confMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, plot the results. For the point-cloud datasets we have two different plot options. We recommend that you start with the normal function `plotResultDots`, which shows the predicted class boundaries and will be easiest to interpret. However, if you want to learn more about how the classifiers work, you can also look at the results from `plotResultsDotsGradient` which shows the strength of each class field as a colored gradient. But this is entirely optional, and will also be more interesting in the multi-layer case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasetNr < 4:\n",
    "    plotResultsDots(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W, B)[1])\n",
    "    #plotResultsDotsGradient(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W, B)[0])\n",
    "else:\n",
    "    plotConfusionMatrixOCR(XTest, LTest, LPredTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2 Optimizing each dataset**\n",
    "\n",
    "The process above is a bit long, since you have to load and normalize data, initailize weights, train the network, measure metrics, and plot the results for each dataset. We can make things a bit easier by defining a function that does most of this for us. In addition to the previous parameters `epochs` and `learningRate`, this function also accepts the parameter `normalize` (True / False) which enables or disables the data normalization. You will use this to answer some questions below. You can also set the optional parameter `useTanhOutput` (True / False) to enable or diable the use of *tanh* activation in the output layer, if you have implemented it in the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSingleLayerOnDataset(datasetNr, testSplit, W0, B0, params):\n",
    "    \"\"\"Train a single layer network on a specific dataset.\n",
    "\n",
    "    Ags:\n",
    "        datasetNr (int): ID of dataset to use\n",
    "        testSplit (float): Fraction of data reserved for testing.\n",
    "        W0 (array): Initial values of the network weights.\n",
    "        B0 (array): Initial values of the network biases.\n",
    "        \n",
    "        params (dict): Dictionary containing:\n",
    "            epochs (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "            normalize (bool): Should data be normalized?\n",
    "            useTanhOutput (bool): Should tanh activation be used for outputs?\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data and split into training and test sets\n",
    "    X, D, L = loadDataset(datasetNr)\n",
    "    D = np.round(D)\n",
    "    XTrain, DTrain, LTrain, XTest, DTest, LTest = splitData(X, D, L, testSplit)\n",
    "\n",
    "    if \"normalize\" in params and params[\"normalize\"]:\n",
    "        XTrainNorm = normalize(XTrain)\n",
    "        XTestNorm  = normalize(XTest)\n",
    "    else:\n",
    "        XTrainNorm = XTrain\n",
    "        XTestNorm  = XTest\n",
    "    \n",
    "    # Train network\n",
    "    W, B, metrics = trainSingleLayer(XTrainNorm, DTrain, XTestNorm, DTest, W0, B0, params, plotProgress=True)\n",
    "\n",
    "    # Predict classes on test set\n",
    "    LPredTrain = forward(XTrainNorm, W, B)[1]\n",
    "    LPredTest = forward(XTestNorm, W, B)[1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    accTrain = calcAccuracy(LPredTrain, LTrain)\n",
    "    accTest = calcAccuracy(LPredTest, LTest)\n",
    "    confMatrix = calcConfusionMatrix(LPredTest, LTest)\n",
    "    \n",
    "    # Display results\n",
    "    print(f'Train accuracy: {accTrain:.4f}')\n",
    "    print(f'Test accuracy: {accTest:.4f}')\n",
    "    print(\"Test data confusion matrix:\")\n",
    "    print(confMatrix)\n",
    "\n",
    "    if datasetNr < 4:\n",
    "        # Switch between these two functions to see another way to visualize the network output.\n",
    "        plotResultsDots(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W, B)[1])\n",
    "        #plotResultsDotsGradient(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W, B)[0])\n",
    "    else:\n",
    "        plotConfusionMatrixOCR(XTest, LTest, LPredTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 Optimizing dataset 1**\n",
    "\n",
    "The first dataset is very simple and you should not have any trouble reaching convergence in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "# ============================================\n",
    "\n",
    "trainSingleLayerOnDataset(1, 0.15, W0, B0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 1:</span>**\n",
    "\n",
    "Optimize the training until you reach at least 98% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 2:</span>**\n",
    "\n",
    "When you have acheived a stable training, run it once with and once without normalization and observe the bias weights in the network plot. Explain your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 Optimizing dataset 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "# ============================================\n",
    "\n",
    "trainSingleLayerOnDataset(2, 0.15, W0, B0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3 Optimizing dataset 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "# ============================================\n",
    "\n",
    "trainSingleLayerOnDataset(3, 0.15, W0, B0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.4 Optimizing dataset 4**\n",
    "\n",
    "Dataset 4 contains OCR-data and is therefore more complicated than the previous three. We therefore recommend that you enable normalization to make the problem a bit easier to solve. We also change the network visualization. Since each weight now corresponds to a pixel in the 8x8 images, we can plot the weights connected to each output class as images as well, which in some sense shows which areas in the images are important to recognize different numbers. It might be a bit difficult to see at first, but somtimes it is possible to see hints of the numbers in these weight illustrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "# ============================================\n",
    "\n",
    "trainSingleLayerOnDataset(4, 0.15, W0, B0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 3:</span>**\n",
    "\n",
    "You should be able to get surprisingly high accuracy on this dataset using the single-layer network. Explain how this is possible.\n",
    "\n",
    "*Hint: Think of the number of input and output dimensions of the problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3. Optional tasks**\n",
    "Here is an optional task that you can try if you are interested to learn more.\n",
    "\n",
    "#### **3.1 Tanh output activations**\n",
    "Implement *tanh* activation in the output layer and re-train the networks in section 2. Do you see any differences in the convergence speed or the decision boundaries? It will probably be more interesting to use the `plotResultsDotsGradient` function for this experiment (see comment at the end of `trainSingleLayerOnDataset`).\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2218871c41807e453fd4062f3c97a84097e40ee8d6d24bb40af7b26f3abea9d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
